{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tweets loaded: 4693094\n",
      "âœ… News loaded: 31037\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import re\n",
    "import warnings\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "tweets_path = \"Bitcoin_tweets.csv\"\n",
    "news_path = \"cryptonews.csv\"\n",
    "df_tweets = dd.read_csv(\n",
    "    tweets_path,\n",
    "    dtype={\n",
    "        \"user_favourites\": \"object\",\n",
    "        \"user_friends\": \"object\",\n",
    "        \"user_followers\": \"object\",\n",
    "        \"user_verified\": \"object\"\n",
    "    },\n",
    "    on_bad_lines=\"skip\",\n",
    "    engine=\"python\",\n",
    "    blocksize=\"256MB\"\n",
    ")\n",
    "df_news = dd.read_csv(\n",
    "    news_path,\n",
    "    dtype={\"title\": \"object\", \"text\": \"object\", \"date\": \"object\"},\n",
    "    on_bad_lines=\"skip\",\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Tweets loaded:\", len(df_tweets))\n",
    "print(\"âœ… News loaded:\", len(df_news))\n",
    "\n",
    "df_tweets[\"date\"] = dd.to_datetime(df_tweets[\"date\"], errors=\"coerce\").dt.floor(\"d\")\n",
    "df_news[\"date\"] = dd.to_datetime(df_news[\"date\"], errors=\"coerce\").dt.floor(\"d\")\n",
    "\n",
    "df_tweets = df_tweets.dropna(subset=[\"date\"])\n",
    "df_news = df_news.dropna(subset=[\"date\"])\n",
    "\n",
    "df_tweets = df_tweets.sample(frac=0.15, random_state=42)\n",
    "df_news = df_news.sample(frac=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa0540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© merging tweets/news (dask)...\n"
     ]
    }
   ],
   "source": [
    "df_tweets = df_tweets.rename(columns={\"text\":\"text_tweet\"}) if \"text\" in df_tweets.columns else df_tweets\n",
    "df_news   = df_news.rename(columns={\"text\":\"text_news\", \"title\":\"title_news\"}) if \"text\" in df_news.columns or \"title\" in df_news.columns else df_news\n",
    "print(\"merging..\")\n",
    "merged = dd.merge(\n",
    "    df_tweets[[\"date\",\"text_tweet\"]].rename(columns={\"text_tweet\":\"text_tweet\"}),\n",
    "    df_news[[\"date\",\"text_news\",\"title_news\"]].rename(columns={\"text_news\":\"text_news\",\"title_news\":\"title_news\"}),\n",
    "    on=\"date\",\n",
    "    how=\"outer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc72141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to compute a sample frac= 0.1\n",
      "Computed merged_small rows: 555901\n",
      "Cleaning texts (simple)...\n"
     ]
    }
   ],
   "source": [
    "merged1 = None\n",
    "merged1 = merged[[\"date\",\"text_tweet\",\"text_news\",\"title_news\"]].sample(frac=0.1, random_state=42).compute()\n",
    "for c in [\"text_tweet\",\"text_news\",\"title_news\"]:\n",
    "    if c not in merged1.columns:\n",
    "        merged1[c] = \"\"\n",
    "    else:\n",
    "        merged1[c] = merged1[c].fillna(\"\")\n",
    "\n",
    "merged1[\"text_all\"] = (\n",
    "merged1[\"text_tweet\"].astype(str) + \" \" +\n",
    "merged1[\"title_news\"].astype(str) + \" \" +\n",
    "merged1[\"text_news\"].astype(str)).str.strip()\n",
    "\n",
    "def clean_text(s, keep_words=200):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"http\\S+\", \" \", s)\n",
    "    s = re.sub(r\"@\\w+\", \" \", s)\n",
    "    s = re.sub(r\"#\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    words = s.split()\n",
    "    if len(words) > keep_words:\n",
    "        words = words[:keep_words]\n",
    "    return \" \".join(words)\n",
    "merged1[\"text_all\"] = merged1[\"text_all\"].apply(clean_text)\n",
    "print(\"text cleaned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating per-day (pandas)...\n",
      "Downloading BTC prices from yfinance...\n",
      "btc processed rows: 1052\n",
      "columns now: ['date', 'close', 'high', 'low', 'open', 'volume', 'next_close', 'y', 'return_1d', 'ma_3', 'ma_7', 'volatility_7d', 'range']\n",
      "        date          open          high           low         close  \\\n",
      "0 2021-01-31  34270.878906  34288.332031  32270.175781  33114.359375   \n",
      "1 2021-02-01  33114.578125  34638.214844  32384.228516  33537.175781   \n",
      "2 2021-02-02  33533.199219  35896.882812  33489.218750  35510.289062   \n",
      "3 2021-02-03  35510.820312  37480.187500  35443.984375  37472.089844   \n",
      "4 2021-02-04  37475.105469  38592.175781  36317.500000  36926.066406   \n",
      "\n",
      "        volume  y  \n",
      "0  52754542671  1  \n",
      "1  61400400660  1  \n",
      "2  63088585433  1  \n",
      "3  61166818159  0  \n",
      "4  68838074392  1  \n",
      "Dataset rows after merge: 650\n",
      "After dropping empty text rows: 650\n",
      "Computing simple sentiment (VADER) per day...\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregating per-day (pandas)...\")\n",
    "daily = merged1.groupby(\"date\").agg(\n",
    "    text_all = (\"text_all\", lambda s: \" \".join(filter(None, map(str, s)))),\n",
    "    tweet_count = (\"text_all\", \"count\")\n",
    ").reset_index()\n",
    "daily[\"avg_word_count\"] = daily[\"text_all\"].apply(lambda t: 0 if not isinstance(t,str) or t.strip()==\"\" else len(t.split()) / max(1, t.count(\" \")+1))\n",
    "print(\"Downloading BTC prices from yfinance...\")\n",
    "start = pd.to_datetime(daily[\"date\"].min()) - pd.Timedelta(days=5)\n",
    "end   = pd.to_datetime(daily[\"date\"].max()) + pd.Timedelta(days=2)\n",
    "btc = yf.download(\"BTC-USD\", start=start, end=end, progress=False).reset_index().rename(columns=str.lower)\n",
    "btc['date'] = pd.to_datetime(btc['date']).dt.floor('d')\n",
    "if isinstance(btc.columns, pd.MultiIndex):\n",
    "    btc.columns = [\n",
    "        \"_\".join([str(i) for i in col if (i is not None and str(i) != \"\")]).strip()\n",
    "        for col in btc.columns\n",
    "    ]\n",
    "if 'date' not in [c.lower() for c in btc.columns]:\n",
    "    btc = btc.reset_index()\n",
    "btc.columns = [str(c).lower().replace(\" \", \"_\") for c in btc.columns]\n",
    "def find_col(key):\n",
    "    for c in btc.columns:\n",
    "        if key in c:\n",
    "            return c\n",
    "    return None\n",
    "col_date   = find_col(\"date\")\n",
    "col_close  = find_col(\"close\")\n",
    "col_open   = find_col(\"open\")\n",
    "col_high   = find_col(\"high\")\n",
    "col_low    = find_col(\"low\")\n",
    "col_volume = find_col(\"volume\")\n",
    "\n",
    "needed = {\"date\":col_date, \"close\":col_close, \"open\":col_open, \"high\":col_high, \"low\":col_low, \"volume\":col_volume}\n",
    "btc[col_close] = pd.to_numeric(btc[col_close], errors=\"coerce\")\n",
    "btc[col_open]  = pd.to_numeric(btc[col_open], errors=\"coerce\")\n",
    "btc[col_high]  = pd.to_numeric(btc[col_high], errors=\"coerce\")\n",
    "btc[col_low]   = pd.to_numeric(btc[col_low], errors=\"coerce\")\n",
    "btc[col_volume]= pd.to_numeric(btc[col_volume], errors=\"coerce\")\n",
    "btc = btc.rename(columns={\n",
    "    col_date: \"date\",\n",
    "    col_close: \"close\",\n",
    "    col_open: \"open\",\n",
    "    col_high: \"high\",\n",
    "    col_low: \"low\",\n",
    "    col_volume: \"volume\"\n",
    "})\n",
    "btc = btc.sort_values(\"date\").reset_index(drop=True)\n",
    "btc = btc.dropna(subset=[\"close\"]).reset_index(drop=True)\n",
    "btc[\"next_close\"] = btc[\"close\"].shift(-1)\n",
    "btc = btc.dropna(subset=[\"next_close\"]).reset_index(drop=True)\n",
    "btc[\"y\"] = (btc[\"next_close\"].values > btc[\"close\"].values).astype(int)\n",
    "btc[\"return_1d\"] = btc[\"close\"].pct_change().fillna(0)\n",
    "btc[\"ma_3\"] = btc[\"close\"].rolling(3).mean().fillna(method=\"bfill\")\n",
    "btc[\"ma_7\"] = btc[\"close\"].rolling(7).mean().fillna(method=\"bfill\")\n",
    "btc[\"volatility_7d\"] = btc[\"return_1d\"].rolling(7).std().fillna(0)\n",
    "btc[\"range\"] = ((btc[\"high\"] - btc[\"low\"]) / btc[\"open\"].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "print(\"btc rows:\", len(btc))\n",
    "print(\"columns:\", btc.columns.tolist())\n",
    "print(btc[['date','open','high','low','close','volume','y']].head())\n",
    "\n",
    "dataset = pd.merge(daily, btc[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"y\",\"return_1d\",\"ma_3\",\"ma_7\",\"volatility_7d\",\"range\"]], on=\"date\", how=\"inner\")\n",
    "dataset = dataset.sort_values(\"date\").reset_index(drop=True)\n",
    "dataset = dataset[dataset[\"text_all\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "print(\"dataset rows:\", dataset.shape[0])\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "dataset[\"sentiment_compound\"] = dataset[\"text_all\"].apply(lambda t: sa.polarity_scores(t)[\"compound\"] if isinstance(t,str) and t.strip() else 0.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ef759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 8, 'min_samples_split': 6, 'n_estimators': 180} Best CV: 0.6213592233009709\n",
      "Final Test acc: 0.6461538461538462\n",
      "thr=0.0065 -> test acc=0.6462\n",
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 3, 'min_samples_split': 11, 'n_estimators': 240} Best CV: 0.5898058252427184\n",
      "Final Test acc: 0.6615384615384615\n",
      "thr=0.0050 -> test acc=0.6615\n",
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 10, 'min_samples_split': 8, 'n_estimators': 160} Best CV: 0.5995145631067961\n",
      "Final Test acc: 0.6384615384615384\n",
      "thr=0.0055 -> test acc=0.6385\n",
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 3, 'min_samples_split': 14, 'n_estimators': 220} Best CV: 0.6067961165048543\n",
      "Final Test acc: 0.6692307692307692\n",
      "thr=0.0060 -> test acc=0.6692\n",
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 180} Best CV: 0.6262135922330098\n",
      "Final Test acc: 0.6538461538461539\n",
      "thr=0.0070 -> test acc=0.6538\n",
      "Fitting 4 folds for each of 1485 candidates, totalling 5940 fits\n",
      "Best params: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 180} Best CV: 0.6286407766990292\n",
      "Final Test acc: 0.6692307692307692\n",
      "thr=0.0075 -> test acc=0.6692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "df = dataset.copy().sort_values(\"date\").reset_index(drop=True)\n",
    "def add_lags_and_rolls(df):\n",
    "    df = df.copy()\n",
    "    for lag in (1,2,3):\n",
    "        df[f\"return_lag{lag}\"] = df[\"return_1d\"].shift(lag).fillna(0)\n",
    "        df[f\"tweet_count_lag{lag}\"] = df[\"tweet_count\"].shift(lag).fillna(0)\n",
    "        df[f\"sentiment_lag{lag}\"] = df[\"sentiment_compound\"].shift(lag).fillna(0)\n",
    "    df[\"sentiment_roll3\"] = df[\"sentiment_compound\"].rolling(3).mean().shift(1).fillna(0)\n",
    "    df[\"tweet_count_roll3\"] = df[\"tweet_count\"].rolling(3).mean().shift(1).fillna(0)\n",
    "    df[\"return_roll3\"] = df[\"return_1d\"].rolling(3).mean().shift(1).fillna(0)\n",
    "    df[\"prev_close\"] = df[\"close\"].shift(1).fillna(method=\"bfill\")\n",
    "    df[\"prev_return\"] = (df[\"close\"] - df[\"prev_close\"]) / df[\"prev_close\"]\n",
    "    df[\"ma_3_diff\"] = df[\"close\"] - df[\"ma_3\"]\n",
    "    df[\"ma_7_diff\"] = df[\"close\"] - df[\"ma_7\"]\n",
    "    return df\n",
    "df = add_lags_and_rolls(df)\n",
    "def build_features(df, tfidf_max=1000, ngram=(1,2), min_df=2):\n",
    "    vec = TfidfVectorizer(max_features=tfidf_max, stop_words=\"english\", ngram_range=ngram, min_df=min_df)\n",
    "    X_text = vec.fit_transform(df[\"text_all\"].astype(str)).toarray()\n",
    "    num_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"return_1d\",\"ma_3\",\"ma_7\",\"volatility_7d\",\"range\",\n",
    "                \"tweet_count\",\"avg_word_count\",\"sentiment_compound\",\n",
    "                \"return_lag1\",\"return_lag2\",\"return_lag3\",\n",
    "                \"tweet_count_lag1\",\"tweet_count_lag2\",\"tweet_count_lag3\",\n",
    "                \"sentiment_lag1\",\"sentiment_lag2\",\"sentiment_lag3\",\n",
    "                \"sentiment_roll3\",\"tweet_count_roll3\",\"return_roll3\",\n",
    "                \"prev_return\",\"ma_3_diff\",\"ma_7_diff\"]\n",
    "    present = [c for c in num_cols if c in df.columns]\n",
    "    X_num = df[present].fillna(0).values\n",
    "    scaler = StandardScaler()\n",
    "    X_num_s = scaler.fit_transform(X_num)\n",
    "    X = np.hstack([X_text, X_num_s])\n",
    "    return X, vec, scaler, present\n",
    "thresholds = [0.0065, 0.005,0.0055,0.006,0.007,0.0075]\n",
    "for thr in thresholds:\n",
    "    df_thr = df.copy()\n",
    "    df_thr[\"next_close\"] = df_thr[\"close\"].shift(-1)\n",
    "    df_thr = df_thr.dropna(subset=[\"next_close\"]).reset_index(drop=True)\n",
    "    df_thr[\"y_thr\"] = ((df_thr[\"next_close\"] - df_thr[\"close\"]) / df_thr[\"close\"] > thr).astype(int)\n",
    "    X, vec, scaler, num_cols_used = build_features(df_thr, tfidf_max=1000, ngram=(1,2), min_df=2)\n",
    "    y = df_thr[\"y_thr\"].values\n",
    "    n = X.shape[0]\n",
    "    split = int(0.8 * n)\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    param_grid ={\"n_estimators\":[160,180,200,220,240,260,280,300 ,320,340,360,380,400,420,440],\n",
    "                \"max_depth\":[3,4,5,6,7,8,9,10,12],\n",
    "                \"min_samples_split\":[4,6,7,8,9,10,11,12,13,14,15]}\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    rf = RandomForestClassifier(class_weight='balanced_subsample', random_state=42, n_jobs=-1)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=tscv, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best = grid.best_estimator_\n",
    "    print(\"Best params:\", grid.best_params_, \"Best CV:\", grid.best_score_)\n",
    "    y_pred = best.predict(X_test)\n",
    "    print(\"Final Test acc:\", accuracy_score(y_test, y_pred))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"thr={thr:.4f} -> test acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13184109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test acc: 0.676923076923077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.81        91\n",
      "           1       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.68       130\n",
      "   macro avg       0.35      0.48      0.40       130\n",
      "weighted avg       0.49      0.68      0.57       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df.copy()\n",
    "df_final[\"next_close\"] = df_final[\"close\"].shift(-1)\n",
    "df_final = df_final.dropna(subset=[\"next_close\"]).reset_index(drop=True)\n",
    "df_final[\"y\"] = ((df_final[\"next_close\"] - df_final[\"close\"]) / df_final[\"close\"] > 0.0075).astype(int)\n",
    "X, vec, scaler, num_cols_used = build_features(df_final, tfidf_max=1500, ngram=(1,2), min_df=2)\n",
    "y = df_final[\"y\"].values\n",
    "split = int(0.8 * len(y))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "rf = RandomForestClassifier(class_weight='balanced_subsample',\n",
    "                            random_state=42,\n",
    "                            max_depth=5,\n",
    "                            min_samples_split=10,\n",
    "                            n_estimators=180,\n",
    "                            n_jobs=-1)\n",
    "rf.fit(X_train , y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Final Test acc:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
